{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ===== Importing Libaries ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\mromanov\\AppData\\Local\\Continuum\\anaconda3\\envs\\ai_fraud\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\mromanov\\AppData\\Local\\Continuum\\anaconda3\\envs\\ai_fraud\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\mromanov\\AppData\\Local\\Continuum\\anaconda3\\envs\\ai_fraud\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\mromanov\\AppData\\Local\\Continuum\\anaconda3\\envs\\ai_fraud\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\mromanov\\AppData\\Local\\Continuum\\anaconda3\\envs\\ai_fraud\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\mromanov\\AppData\\Local\\Continuum\\anaconda3\\envs\\ai_fraud\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "from sklearn.utils import class_weight\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "import shap\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Importing from other files\n",
    "from text_processing_cleanup import text_Processing, text_Processing_GloVe\n",
    "from data_feature_functions import create_indicator_matrix, get_Fraud_Dataset,get_training_testing_data, get_Geolocation_Data, print_class_report_confusion_matrix, get_combined_feature, export_predictions\n",
    "from predict_CNN_1layer import get_cnn_pred_prob, evaluate_cnn\n",
    "from ensemble_classifier import train_bagging_ensemble, train_boosting_ensemble, perfrom_GridSearch, perfrom_RandomSearch\n",
    "from word_vector_functions import  load_word2vec, get_sentence_feature_values, read_glove_file, get_sentence_embeddings\n",
    "from get_classifier_models import get_classifier_predictions_probabilities\n",
    "from data_visualization import plot_3d, plot_decision_regions, get_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 38 # out of 100 percent for train-test split\n",
    "test_percentage = float(test_size / 100.00)\n",
    "rand_state = 47 # random state of train/test split\n",
    "embedding_dim  = 200\n",
    "\n",
    "text_type = \"Clean_Nums_2char\"\n",
    "glove_sum = \"VecAvg\"\n",
    "recreate_full_xgb = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting fraud data and GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data path: C:\\Current_Projects\\Fraud_Project_AI\\Initial_Datasets \n",
      " Local Fraud Data Path: C:\\Current_Projects\\Fraud_Project_AI\\Initial_Datasets\\MasterFraudData_more datav2.xlsx \n",
      " Feature Gen Path: C:\\Current_Projects\\Fraud_Project_AI\\Feature_Generated_Datasets\\Fraud_Features_NoLossDate_Refferal.xlsx\n",
      "Initial Large dataframe size before drop 79432\n",
      "\n",
      "Initial Large dataframe size after drop 17066\n",
      "\n",
      "Type of zip code data: <class 'pandas.core.frame.DataFrame'>\n",
      "\n",
      "Loading Lat/Long\n",
      "Type of city data: <class 'pandas.core.frame.DataFrame'>\n",
      "Initial Large dataframe size after merge 17066\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'multi_body_parts_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-320f1db4ef52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfraud_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_Fraud_Dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecreate_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraud_type\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'acceptance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfraud_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fraud_Text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# features or inputs into model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfraud_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Fraud_Label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Current_Projects\\Fraud_Project_AI\\data_feature_functions.py\u001b[0m in \u001b[0;36mget_Fraud_Dataset\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m         \u001b[1;31m# generating features with helper function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m         \u001b[0mfeature_data_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_Generation_Init_Data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_Dateset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_large_data_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"sublineBusiness\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malready_corrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m         \u001b[0mfeature_data_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_data_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Current_Projects\\Fraud_Project_AI\\data_feature_functions.py\u001b[0m in \u001b[0;36mfeature_Generation_Init_Data\u001b[1;34m(initial_Dateset, feature_type, already_corrected)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Size of Date Diff Dataframe {0}\\n Size of Body Dataframe {1}\\n Size of New Text Dataframe {2}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_diff_features_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_body_parts_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_text_feature_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'multi_body_parts_feature' is not defined"
     ]
    }
   ],
   "source": [
    "fraud_data = get_Fraud_Dataset(recreate_features = False, fraud_type= 'acceptance')\n",
    "\n",
    "text = fraud_data['Fraud_Text'] # features or inputs into model\n",
    "labels = fraud_data['Fraud_Label'] # labels\n",
    "\n",
    "labels_onehot = create_indicator_matrix(fraud_data['Fraud_Label'], check_index = True)\n",
    "\n",
    "\n",
    "cleaned_text = text.apply(text_Processing, numbers=False)\n",
    "\n",
    "text_word_vec = text.apply(text_Processing_GloVe)\n",
    "\n",
    "clean_list = cleaned_text.tolist()\n",
    "\n",
    "clean_wordVec_list = text_word_vec.tolist()\n",
    "\n",
    "glove_dict = read_glove_file(data_type = \"accept\", dimension = embedding_dim,  vocab_size = \"4k\", glove_type = text_type)\n",
    "\n",
    "\n",
    "new_feat_avg = fraud_data.loc[:, 'Loss_PolicyEff':'Claim_Loss',] # Datediff features + fraud text\n",
    "new_feature_datediff = fraud_data.loc[:, 'Loss_PolicyEff':'Claim_Loss'] # Datediff features\n",
    "\n",
    "\n",
    "new_feature_labels = fraud_data['Fraud_Label']\n",
    "fraud_data['Main Cause'].fillna(\"\", inplace=True)\n",
    "fraud_data['Sub Cause'].fillna(\"\", inplace=True)\n",
    "fraud_data['New_City'].fillna(\"\", inplace=True)\n",
    "fraud_data['Longitude'].fillna(0, inplace=True)\n",
    "fraud_data['Latitude'].fillna(0, inplace=True)\n",
    "\n",
    "fraud_data['New_Main'] = fraud_data['Main Cause']\n",
    "fraud_data['New_Sub'] = fraud_data['Sub Cause']\n",
    "fraud_data['Sub_Business'] = fraud_data['Subline_Business']\n",
    "fraud_data['Cause'] = fraud_data['Main Cause'] + \" - \" + fraud_data['Sub Cause']\n",
    "fraud_data['Cause_Join'] = fraud_data['Main Cause'] + \" - \" + fraud_data['Sub Cause']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding features and getting Glove Average for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding main/sub cause columns\n",
    "encoder = ce.leave_one_out.LeaveOneOutEncoder(cols = ['Cause',  'Sub_Business', 'New_Main', 'New_Sub', 'New_City'])\n",
    "\n",
    "\n",
    "# encoder = ce.BackwardDifferenceEncoder(cols = ['New_Main', 'New_Sub'])\n",
    "\n",
    "fraud_data = encoder.fit_transform(X=fraud_data, y=fraud_data['Fraud_Label'])\n",
    "\n",
    "\n",
    "\n",
    "feature_cols = [\n",
    "\n",
    "               # 'Loss_PolicyEff',\n",
    "               'Loss_PolicyExp',\n",
    "                'Claim_PolicyEff',\n",
    "                'Claim_Loss',\n",
    "                'Longitude',\n",
    "                'Latitude',\n",
    "                'Sub_Business',\n",
    "                'Cause']\n",
    "\n",
    "new_feat_avg = fraud_data[feature_cols]\n",
    "\n",
    "loss_cleaned = fraud_data['Fraud_Text'].apply(text_Processing, numbers=False)\n",
    "\n",
    "sentence_embedding_avg = np.array( [get_sentence_feature_values(sentence = words, embedding = glove_dict, embedding_dim = 200) for words in loss_cleaned ])\n",
    "\n",
    "sent_orig_sum = sentence_embedding_avg.sum(axis = 1) # getting sum of sentece embeddings along row\n",
    "\n",
    "sent_embed_loss = get_sentence_embeddings(text = loss_cleaned, embedding = glove_dict, embedding_size = embedding_dim)\n",
    "\n",
    "sent_sum_loss = [sent.sum() for sent in sent_embed_loss]\n",
    "\n",
    "sent_avg_loss = [np.average(sent) for sent in sent_embed_loss]\n",
    "\n",
    "\n",
    "features_orig = [\n",
    "                   # 'Loss_PolicyEff',\n",
    "                   'Loss_PolicyExp',\n",
    "                    'Claim_PolicyEff',\n",
    "                    'Claim_Loss',\n",
    "                    'Longitude',\n",
    "                    'Latitude',\n",
    "                    'Subline_Business',\n",
    "                    'Cause_Join',\n",
    "                    'Loss_Description'\n",
    "                    ]\n",
    "new_feat_orig = fraud_data[features_orig]\n",
    "\n",
    "new_feat_avg['Loss_Descrip'] = sent_avg_loss\n",
    "\n",
    "fraud_data['Loss_Glove_Avg'] = sent_avg_loss\n",
    "\n",
    "new_feat_orig.rename(\n",
    "                columns= {\n",
    "                    'Main Cause': 'Main_Cause',\n",
    "                    'Sub Cause': 'Sub_Cause',\n",
    "                    'Loss_Description': 'Loss_Descrip'\n",
    "                },\n",
    "                inplace=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataframe with 200 dimensional Glove Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_embed_df = pd.DataFrame(sent_embed_loss, columns = ['Loss_Descrip_Feat_' + str(feat) for feat in range( len(sent_embed_loss[0]) ) ] )\n",
    "\n",
    "sent_embed_df.set_index(new_feature_datediff.index, inplace=True)\n",
    "\n",
    "\n",
    "new_feature_expand = pd.concat([new_feature_datediff, fraud_data['Cause'],  fraud_data['Longitude'], fraud_data['Latitude']], axis=1)\n",
    "\n",
    "new_feature_expand = new_feature_expand.join([sent_embed_df ], how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_avg, X_test_avg, y_train_avg, y_test_avg = train_test_split(new_feat_avg, new_feature_labels, test_size = test_percentage, random_state = rand_state)\n",
    "\n",
    "X_train_expand, X_test_expand, y_train_expand, y_test_expand = train_test_split(new_feature_expand, new_feature_labels, test_size = test_percentage, random_state = rand_state)\n",
    "\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(new_feat_orig, new_feature_labels, test_size = test_percentage, random_state = rand_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training xgboost model and exporting predictions to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_pred, boost_prob, boost_model, boost_pred_path = train_boosting_ensemble(X_train_avg, X_test_avg, y_train_avg, y_test_avg,\n",
    "                                                                                  boosting_type= \"xgboost_{}test_{}_FullData\".format(test_size, 'VecAvg'),\n",
    "                                                                                  recreate_model= True, model_type = 'imbalanced')\n",
    "\n",
    "boost_pred_expand, boost_prob_expand, boost_model_expand, boost_pred_path_expand = train_boosting_ensemble(X_train_expand, X_test_expand, y_train_expand, y_test_expand,\n",
    "                                                                                  boosting_type= \"xgboost_{}test_Glove{}_FullData\".format(test_size, 'Expand'),\n",
    "                                                                                  recreate_model= True, model_type = 'imbalanced')\n",
    "    \n",
    "class_xgb =  print_class_report_confusion_matrix(y_test_avg, boost_pred, \"XGBoost\", \"Glove Sum Full Data\")\n",
    "\n",
    "class_xgb_expand =  print_class_report_confusion_matrix(y_test_expand, boost_pred_expand, \"XGBoost\", \"Glove Expand Full Data\")\n",
    "\n",
    "\n",
    "boost_out = export_predictions(\n",
    "                  fraud_data,\n",
    "                  boost_prob,\n",
    "                  boost_pred,\n",
    "                  actual= y_test_avg,\n",
    "                  recreate_ProbPreds = True,\n",
    "                  pred_path = boost_pred_path,\n",
    "                  file_name = 'XGBoost_Output_{}test_{}{}_FullData'.format(test_size, text_type, glove_sum),\n",
    "                  model_type ='XGBoost')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting xgboost outputs and getting feature impact/importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data_points = { 'x':boost_out['Actual Label'] , 'y':boost_out['XGBoost_Predictions'] , 'z': boost_out['XGBoost_Confid_Prob'] * 100.00}\n",
    "# plot_3d(model_output= boost_out,  data_points = plot_data_points, fig_type = \"Boost_FullData_Actual_Pred_Prob\", model_type = \"XGBoost\",  z_label = \"Probability Fraud (%)\")\n",
    "\n",
    "\n",
    "# plot_data_points = { 'x':boost_out['Actual Label'] , 'y':boost_out['XGBoost_Predictions'] , 'z': boost_out['Claim_Loss'] }\n",
    "# plot_3d(model_output= boost_out,  data_points = plot_data_points, fig_type = \"Boost_Full_Actual_Pred_ClaimLoss\", model_type = \"XGBoost\", z_label = \"Days Between Policy Loss - Claim\")\n",
    "\n",
    "# get_feature_importance(model = boost_model,\n",
    "#                            features = X_test_avg,\n",
    "#                            feature_names = list(X_test_avg.columns),\n",
    "#                            orig_feat = X_test_orig, \n",
    "#                            model_type = 'xgb')\n",
    "\n",
    "get_feature_importance(model = boost_model,\n",
    "                           features = X_test_avg,\n",
    "                           feature_names = list(X_test_avg.columns),\n",
    "                           orig_feat = X_test_orig, \n",
    "                           model_type = 'xgb',\n",
    "                           plot = 'decision')\n",
    "\n",
    "get_feature_importance(model = boost_model,\n",
    "                           features = X_test_avg,\n",
    "                           feature_names = list(X_test_avg.columns),\n",
    "                           orig_feat = X_test_orig, \n",
    "                           model_type = 'xgb',\n",
    "                           plot = 'force')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting CNN output and combining output to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred, cnn_prob, cnn_out_path = get_cnn_pred_prob(cleaned_text = cleaned_text, labels = labels_onehot,\n",
    "                                                         testing = True,\n",
    "                                                         prob_type='{}test'.format(test_size),\n",
    "                                                         test_size = test_size,\n",
    "                                                         rand_state = rand_state,\n",
    "                                                         glove_type = text_type,\n",
    "                                                         embedding_dim = embedding_dim,\n",
    "                                                         recreate_prob = False)\n",
    "\n",
    "\n",
    "# evaluate_cnn(cleaned_text = cleaned_text, labels = labels_onehot)\n",
    "\n",
    "new_feature = get_combined_feature(\n",
    "                                features =  fraud_data,\n",
    "                                cnn_Prob = cnn_out_path,\n",
    "                                recreate_combined_features = True,\n",
    "                                feature_type = '{}test_{}_{}D'.format(test_size, text_type, embedding_dim)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "new_feature_datediff = new_feature.loc[:, 'Loss_PolicyEff':'Multi_Body_Parts_Injured'] # Datediff features\n",
    "new_feat_avg = new_feature.loc[:, 'Loss_PolicyEff':'Multi_Body_Parts_Injured'] # Datediff features + fraud text\n",
    "new_feature = new_feature.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "cnn_features = [\n",
    "                 # 'Loss_PolicyEff',\n",
    "                 'Loss_PolicyExp',\n",
    "                  'Claim_PolicyEff',\n",
    "                  'Claim_Loss',\n",
    "                  'New_Main',\n",
    "                  'New_Sub',\n",
    "                  'Longitude',\n",
    "                  'Latitude',\n",
    "                  'Sub_Business',\n",
    "                  'CNN_Prob_Fraud'\n",
    "            ]\n",
    "\n",
    "new_feature_xgb = new_feature[cnn_features]\n",
    "\n",
    "new_feature_labels = new_feature['Fraud_Label']\n",
    "new_feature['Main Cause'].fillna(\"\", inplace=True)\n",
    "new_feature['Sub Cause'].fillna(\"\", inplace=True)\n",
    "new_feature['New_City'].fillna(\"\", inplace=True)\n",
    "new_feature['Longitude'].fillna(0, inplace=True)\n",
    "new_feature['Latitude'].fillna(0, inplace=True)\n",
    "\n",
    "loss_cleaned = new_feature['Loss_Descrip_NoAddition'].apply(text_Processing, numbers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
